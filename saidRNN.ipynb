{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'abstract', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data – update the file paths if needed\n",
    "train_df = pd.read_csv(\"arxiv_train.csv\")\n",
    "test_df = pd.read_csv(\"arxiv_test.csv\")\n",
    "\n",
    "# Quick check of columns\n",
    "print(train_df.columns)\n",
    "# Expected output: Index(['Unnamed: 0', 'abstract', 'label', 'clean_abstract'], dtype='object')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Using Keras Tokenizer to Convert Text to Sequences\n",
    "\n",
    "Since the \"clean_abstract\" column contains strings, we can directly use it with the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "Tokenizer    = tf.keras.preprocessing.text.Tokenizer\n",
    "pad_sequences= tf.keras.preprocessing.sequence.pad_sequences\n",
    "\n",
    "# Set a vocabulary size and an out-of-vocabulary token\n",
    "vocab_size = 10000  # you can adjust based on your data\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "\n",
    "# Fit the tokenizer on the training abstracts\n",
    "tokenizer.fit_on_texts(train_df[\"clean_abstract\"])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"clean_abstract\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"clean_abstract\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Padding the Sequences\n",
    "\n",
    "Choose a maximum sequence length (e.g., based on some percentile of abstract lengths) and pad the sequences so that each input has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200  # adjust as needed based on your data distribution\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Converting Labels to a Suitable Format\n",
    "\n",
    "If your labels are given as strings (for example, like \"eess\", etc.), convert them to integer values using scikit-learn’s LabelEncoder, and then to one-hot encoding if needed.\n",
    "a. Encode String Labels as Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(train_df[\"label\"])\n",
    "y_test_encoded = le.transform(test_df[\"label\"])\n",
    "\n",
    "# Optional: print label mapping to see the conversion\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Convert to One-Hot Encoding (for multi-class classification)\n",
    "\n",
    "If you’re using a categorical crossentropy loss, convert your integer labels to one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "y_train_cat = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Building an RNN-Based Model\n",
    "\n",
    "Now that we have the padded sequences and the encoded labels, we can build our RNN-based classifier. In this step, we’ll demonstrate a simple model using an LSTM layer. The LSTM will process the sequence and return its last hidden state as a fixed-size representation, which is then fed into dense layers to do the classification.\n",
    "a. Example 1: Using a Single LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "Sequential   = tf.keras.models.Sequential\n",
    "Dense        = tf.keras.layers.Dense\n",
    "LSTM         = tf.keras.layers.LSTM\n",
    "Embedding    = tf.keras.layers.Embedding\n",
    "Dropout      = tf.keras.layers.Dropout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embedding_dim = 100  # you can adjust (or use a pretrained embedding matrix if desired)\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding layer: learns an embedding for each word\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "# LSTM layer; note that it returns only the final state by default (return_sequences=False)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# Optional additional dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# Final output layer; using softmax activation for multi-class classification\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Example 2: Using a Bidirectional LSTM\n",
    "\n",
    "A Bidirectional LSTM processes the sequence in both forward and backward directions. You can then either take the last hidden state, or concatenate states from both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "Bidirectional= tf.keras.layers.Bidirectional\n",
    "\n",
    "\n",
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "# Bidirectional LSTM layer; automatically concatenates forward and backward states\n",
    "model_bi.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model_bi.add(Dense(64, activation='relu'))\n",
    "model_bi.add(Dropout(0.5))\n",
    "model_bi.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_bi.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_bi.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training the Model\n",
    "\n",
    "Train the model using the padded sequences and one-hot encoded labels. For demonstration, here’s how you would train the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10   # Adjust depending on your dataset size and overfitting\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train_cat, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_cat))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can similarly train the bidirectional model by replacing model with model_bi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluating the Model and Extracting the Fixed-Size Representation\n",
    "\n",
    "After training, evaluate the model’s performance on the test set. You can also predict on the test set and use the final layer outputs to understand how well your RNN captured the representations.\n",
    "a. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Making Predictions and Comparing Different Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Obtain predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compare to integer encoded true labels\n",
    "print(classification_report(y_test_encoded, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Summary and Next Steps\n",
    "\n",
    "    Tokenization & Padding: Use Keras’ Tokenizer to convert text to sequences and pad them to a fixed length.\n",
    "\n",
    "    Label Encoding: Convert string labels to integers (and then to one-hot vectors if needed).\n",
    "\n",
    "    RNN-Based Model:\n",
    "\n",
    "        Use an Embedding layer followed by an LSTM (or GRU) layer.\n",
    "\n",
    "        For a fixed-size representation, use the final hidden state of the RNN (or use a Bidirectional RNN which concatenates states).\n",
    "\n",
    "    Training and Evaluation: Train your model and evaluate using accuracy and detailed classification reports.\n",
    "\n",
    "    Experiment: Switch between LSTM, GRU, and Bidirectional RNNs, and adjust hyperparameters (layer sizes, dropout rates) to see which architecture performs best on your dataset.\n",
    "\n",
    "This modular approach lets you easily compare the different RNN architectures and pooling strategies for extracting document-level representations, allowing you to select the best model for your classification task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
