{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('arxiv_train.csv')\n",
    "test_df = pd.read_csv('arxiv_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting preprocessing you lame ass nigga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train missing values:\n",
      " Unnamed: 0    0\n",
      "abstract      0\n",
      "label         0\n",
      "dtype: int64\n",
      "Test missing values:\n",
      " Unnamed: 0    0\n",
      "abstract      0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train missing values:\\n\", train_df.isnull().sum())\n",
    "print(\"Test missing values:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['abstract', 'label'], inplace=True)\n",
    "test_df.dropna(subset=['abstract', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing extra spaces, new lines, tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_df['clean_abstract'] = train_df['abstract'].apply(clean_text)\n",
    "test_df['clean_abstract'] = test_df['abstract'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31716</td>\n",
       "      <td>Automatic meeting analysis is an essential f...</td>\n",
       "      <td>eess</td>\n",
       "      <td>automatic meeting analysis is an essential fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89533</td>\n",
       "      <td>We propose a protocol to encode classical bi...</td>\n",
       "      <td>quant-ph</td>\n",
       "      <td>we propose a protocol to encode classical bits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82700</td>\n",
       "      <td>A number of physically intuitive results for...</td>\n",
       "      <td>quant-ph</td>\n",
       "      <td>a number of physically intuitive results for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78830</td>\n",
       "      <td>In the last decade rare-earth hexaborides ha...</td>\n",
       "      <td>physics</td>\n",
       "      <td>in the last decade rare-earth hexaborides have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94948</td>\n",
       "      <td>We introduce the weak barycenter of a family...</td>\n",
       "      <td>stat</td>\n",
       "      <td>we introduce the weak barycenter of a family o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract     label  \\\n",
       "0       31716    Automatic meeting analysis is an essential f...      eess   \n",
       "1       89533    We propose a protocol to encode classical bi...  quant-ph   \n",
       "2       82700    A number of physically intuitive results for...  quant-ph   \n",
       "3       78830    In the last decade rare-earth hexaborides ha...   physics   \n",
       "4       94948    We introduce the weak barycenter of a family...      stat   \n",
       "\n",
       "                                      clean_abstract  \n",
       "0  automatic meeting analysis is an essential fun...  \n",
       "1  we propose a protocol to encode classical bits...  \n",
       "2  a number of physically intuitive results for t...  \n",
       "3  in the last decade rare-earth hexaborides have...  \n",
       "4  we introduce the weak barycenter of a family o...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoding labels as integers because its a multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4, 'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# Save the mapping for later use\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stratified Train/Dev Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      " label_encoded\n",
      "9    0.100687\n",
      "3    0.100656\n",
      "7    0.100281\n",
      "4    0.100125\n",
      "2    0.100062\n",
      "0    0.099844\n",
      "8    0.099828\n",
      "5    0.099766\n",
      "6    0.099469\n",
      "1    0.099281\n",
      "Name: proportion, dtype: float64\n",
      "Dev label distribution:\n",
      " label_encoded\n",
      "9    0.100687\n",
      "3    0.100625\n",
      "7    0.100312\n",
      "4    0.100125\n",
      "2    0.100062\n",
      "8    0.099875\n",
      "0    0.099812\n",
      "5    0.099750\n",
      "6    0.099500\n",
      "1    0.099250\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split on the encoded labels\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    train_df['clean_abstract'],\n",
    "    train_df['label_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label_encoded']\n",
    ")\n",
    "\n",
    "# Optional sanity check: label distribution\n",
    "print(\"Train label distribution:\\n\", train_labels.value_counts(normalize=True))\n",
    "print(\"Dev label distribution:\\n\", dev_labels.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels.values\n",
    "y_dev = dev_labels.values\n",
    "label_map = {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4,\n",
    "             'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1 : text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basic count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer shape (train): (64000, 10000)\n",
      "CountVectorizer shape (dev): (16000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_count = count_vectorizer.fit_transform(train_texts)\n",
    "X_dev_count = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "print(\"CountVectorizer shape (train):\", X_train_count.shape)\n",
    "print(\"CountVectorizer shape (dev):\", X_dev_count.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (64000, 10000)\n",
      "TF-IDF shape (dev): (16000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(dev_texts)\n",
    "\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF shape (dev):\", X_dev_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting data to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this for BOW tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense and then to tensors\n",
    "X_train_tensor = torch.tensor(X_train_count.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_count.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this for TF-IDF tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_tfidf.toarray(), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- definfing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))  # Regularization\n",
    "            current_dim = h\n",
    "        layers.append(nn.Linear(current_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 831.5841\n",
      "Epoch 2/5 - Loss: 439.9397\n",
      "Epoch 3/5 - Loss: 336.5091\n",
      "Epoch 4/5 - Loss: 258.9744\n",
      "Epoch 5/5 - Loss: 196.4470\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FeedforwardNN(input_dim=10000, hidden_dims=[128, 64], num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results with BOW tensors \n",
    "- Epoch 1/5 - Loss: 706.6508\n",
    "- Epoch 2/5 - Loss: 390.9463\n",
    "- Epoch 3/5 - Loss: 277.9484\n",
    "- Epoch 4/5 - Loss: 196.3589\n",
    "- Epoch 5/5 - Loss: 142.3870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rusults with TF-IDF tensors \n",
    "- Epoch 1/5 - Loss: 831.5841\n",
    "- Epoch 2/5 - Loss: 439.9397\n",
    "- Epoch 3/5 - Loss: 336.5091\n",
    "- Epoch 4/5 - Loss: 258.9744\n",
    "- Epoch 5/5 - Loss: 196.4470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.95      0.93      0.94      1597\n",
      "    cond-mat       0.81      0.78      0.79      1588\n",
      "          cs       0.71      0.68      0.70      1601\n",
      "        eess       0.76      0.82      0.79      1610\n",
      "      hep-ph       0.93      0.93      0.93      1602\n",
      "      hep-th       0.91      0.90      0.91      1596\n",
      "        math       0.83      0.85      0.84      1592\n",
      "     physics       0.70      0.67      0.68      1605\n",
      "    quant-ph       0.84      0.87      0.86      1598\n",
      "        stat       0.80      0.81      0.81      1611\n",
      "\n",
      "    accuracy                           0.82     16000\n",
      "   macro avg       0.82      0.83      0.82     16000\n",
      "weighted avg       0.82      0.82      0.82     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in dev_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compraing BOW and TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Metric  CountVectorizer  TF-IDF\n",
      "   Accuracy             0.83    0.82\n",
      "   Macro F1             0.83    0.82\n",
      "Weighted F1             0.83    0.82\n",
      "Astro-ph F1             0.93    0.94\n",
      "Cond-mat F1             0.80    0.79\n",
      "      CS F1             0.70    0.70\n",
      "    EESS F1             0.79    0.79\n",
      "  Hep-ph F1             0.93    0.93\n",
      "  Hep-th F1             0.91    0.91\n",
      "    Math F1             0.85    0.84\n",
      " Physics F1             0.69    0.68\n",
      "Quant-ph F1             0.86    0.86\n",
      "    Stat F1             0.80    0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the metrics for both models\n",
    "data = {\n",
    "    \"Metric\": [\n",
    "        \"Accuracy\", \"Macro F1\", \"Weighted F1\", \n",
    "        \"Astro-ph F1\", \"Cond-mat F1\", \"CS F1\", \"EESS F1\", \"Hep-ph F1\", \n",
    "        \"Hep-th F1\", \"Math F1\", \"Physics F1\", \"Quant-ph F1\", \"Stat F1\"\n",
    "    ],\n",
    "    \"CountVectorizer\": [\n",
    "        0.83, 0.83, 0.83,\n",
    "        0.93, 0.80, 0.70, 0.79, 0.93,\n",
    "        0.91, 0.85, 0.69, 0.86, 0.80\n",
    "    ],\n",
    "    \"TF-IDF\": [\n",
    "        0.82, 0.82, 0.82,\n",
    "        0.94, 0.79, 0.70, 0.79, 0.93,\n",
    "        0.91, 0.84, 0.68, 0.86, 0.81\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the table\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next steps (for said)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You're in a great spot to move to Step 2: Pretrained Word Embeddings. Here's what you could try next:\n",
    "\n",
    "- Replace BoW with averaged GloVe or word2vec embeddings for each abstract.\n",
    "\n",
    "- Use mean/sum/max pooling over word embeddings to create document vectors.\n",
    "\n",
    "- Feed those vectors into the same MLP structure for a fair comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
