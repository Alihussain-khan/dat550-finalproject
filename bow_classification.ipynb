{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('arxiv_train.csv')\n",
    "test_df = pd.read_csv('arxiv_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting preprocessing you lame ass nigga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train missing values:\n",
      " Unnamed: 0    0\n",
      "abstract      0\n",
      "label         0\n",
      "dtype: int64\n",
      "Test missing values:\n",
      " Unnamed: 0    0\n",
      "abstract      0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train missing values:\\n\", train_df.isnull().sum())\n",
    "print(\"Test missing values:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['abstract', 'label'], inplace=True)\n",
    "test_df.dropna(subset=['abstract', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing extra spaces, new lines, tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_df['clean_abstract'] = train_df['abstract'].apply(clean_text)\n",
    "test_df['clean_abstract'] = test_df['abstract'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'abstract', 'label', 'clean_abstract'], dtype='object')\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# we dont need it!!!!!!!\n",
    "train_df.head()\n",
    "print(train_df.columns)\n",
    "# Check the type of the first element in the 'clean_abstract' column\n",
    "print(type(train_df['clean_abstract'].iloc[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Apply the Tokenization to Your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      clean_abstract  \\\n",
      "0  automatic meeting analysis is an essential fun...   \n",
      "1  we propose a protocol to encode classical bits...   \n",
      "2  a number of physically intuitive results for t...   \n",
      "3  in the last decade rare-earth hexaborides have...   \n",
      "4  we introduce the weak barycenter of a family o...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [automatic, meeting, analysis, is, an, essenti...  \n",
      "1  [we, propose, a, protocol, to, encode, classic...  \n",
      "2  [a, number, of, physically, intuitive, results...  \n",
      "3  [in, the, last, decade, rare, earth, hexaborid...  \n",
      "4  [we, introduce, the, weak, barycenter, of, a, ...  \n",
      "<class 'str'>\n",
      "Index(['Unnamed: 0', 'abstract', 'label', 'clean_abstract', 'tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    # use re.findall to grab sequences of word characters (letters/numbers)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text) \n",
    "    return tokens\n",
    "\n",
    "train_df['tokens'] = train_df['clean_abstract'].apply(simple_tokenize)\n",
    "test_df['tokens'] = test_df['clean_abstract'].apply(simple_tokenize)\n",
    "\n",
    "print(train_df[['clean_abstract', 'tokens']].head())\n",
    "print(type(train_df['clean_abstract'].iloc[0]))\n",
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoding labels as integers because its a multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4, 'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# Save the mapping for later use\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stratified Train/Dev Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      " label_encoded\n",
      "9    0.100687\n",
      "3    0.100656\n",
      "7    0.100281\n",
      "4    0.100125\n",
      "2    0.100062\n",
      "0    0.099844\n",
      "8    0.099828\n",
      "5    0.099766\n",
      "6    0.099469\n",
      "1    0.099281\n",
      "Name: proportion, dtype: float64\n",
      "Dev label distribution:\n",
      " label_encoded\n",
      "9    0.100687\n",
      "3    0.100625\n",
      "7    0.100312\n",
      "4    0.100125\n",
      "2    0.100062\n",
      "8    0.099875\n",
      "0    0.099812\n",
      "5    0.099750\n",
      "6    0.099500\n",
      "1    0.099250\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split on the encoded labels\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    train_df['clean_abstract'],\n",
    "    train_df['label_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label_encoded']\n",
    ")\n",
    "\n",
    "# Optional sanity check: label distribution\n",
    "print(\"Train label distribution:\\n\", train_labels.value_counts(normalize=True))\n",
    "print(\"Dev label distribution:\\n\", dev_labels.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels.values\n",
    "y_dev = dev_labels.values\n",
    "label_map = {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4,\n",
    "             'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1 : text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basic count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer shape (train): (64000, 10000)\n",
      "CountVectorizer shape (dev): (16000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_count = count_vectorizer.fit_transform(train_texts)\n",
    "X_dev_count = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "print(\"CountVectorizer shape (train):\", X_train_count.shape)\n",
    "print(\"CountVectorizer shape (dev):\", X_dev_count.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (64000, 10000)\n",
      "TF-IDF shape (dev): (16000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(dev_texts)\n",
    "\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF shape (dev):\", X_dev_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting data to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this for BOW tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense and then to tensors\n",
    "X_train_tensor = torch.tensor(X_train_count.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_count.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this for TF-IDF tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_tfidf.toarray(), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- definfing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))  # Regularization\n",
    "            current_dim = h\n",
    "        layers.append(nn.Linear(current_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 824.0674\n",
      "Epoch 2/5 - Loss: 437.6404\n",
      "Epoch 3/5 - Loss: 331.6392\n",
      "Epoch 4/5 - Loss: 255.8558\n",
      "Epoch 5/5 - Loss: 196.6918\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FeedforwardNN(input_dim=10000, hidden_dims=[128, 64], num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results with BOW tensors \n",
    "- Epoch 1/5 - Loss: 706.6508\n",
    "- Epoch 2/5 - Loss: 390.9463\n",
    "- Epoch 3/5 - Loss: 277.9484\n",
    "- Epoch 4/5 - Loss: 196.3589\n",
    "- Epoch 5/5 - Loss: 142.3870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rusults with TF-IDF tensors \n",
    "- Epoch 1/5 - Loss: 831.5841\n",
    "- Epoch 2/5 - Loss: 439.9397\n",
    "- Epoch 3/5 - Loss: 336.5091\n",
    "- Epoch 4/5 - Loss: 258.9744\n",
    "- Epoch 5/5 - Loss: 196.4470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.93      0.93      0.93      1597\n",
      "    cond-mat       0.78      0.82      0.80      1588\n",
      "          cs       0.71      0.68      0.69      1601\n",
      "        eess       0.76      0.83      0.79      1610\n",
      "      hep-ph       0.95      0.91      0.93      1602\n",
      "      hep-th       0.89      0.92      0.91      1596\n",
      "        math       0.86      0.83      0.84      1592\n",
      "     physics       0.68      0.67      0.68      1605\n",
      "    quant-ph       0.88      0.83      0.86      1598\n",
      "        stat       0.80      0.83      0.81      1611\n",
      "\n",
      "    accuracy                           0.82     16000\n",
      "   macro avg       0.82      0.82      0.82     16000\n",
      "weighted avg       0.82      0.82      0.82     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in dev_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compraing BOW and TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Metric  CountVectorizer  TF-IDF\n",
      "   Accuracy             0.83    0.82\n",
      "   Macro F1             0.83    0.82\n",
      "Weighted F1             0.83    0.82\n",
      "Astro-ph F1             0.93    0.94\n",
      "Cond-mat F1             0.80    0.79\n",
      "      CS F1             0.70    0.70\n",
      "    EESS F1             0.79    0.79\n",
      "  Hep-ph F1             0.93    0.93\n",
      "  Hep-th F1             0.91    0.91\n",
      "    Math F1             0.85    0.84\n",
      " Physics F1             0.69    0.68\n",
      "Quant-ph F1             0.86    0.86\n",
      "    Stat F1             0.80    0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the metrics for both models\n",
    "data = {\n",
    "    \"Metric\": [\n",
    "        \"Accuracy\", \"Macro F1\", \"Weighted F1\", \n",
    "        \"Astro-ph F1\", \"Cond-mat F1\", \"CS F1\", \"EESS F1\", \"Hep-ph F1\", \n",
    "        \"Hep-th F1\", \"Math F1\", \"Physics F1\", \"Quant-ph F1\", \"Stat F1\"\n",
    "    ],\n",
    "    \"CountVectorizer\": [\n",
    "        0.83, 0.83, 0.83,\n",
    "        0.93, 0.80, 0.70, 0.79, 0.93,\n",
    "        0.91, 0.85, 0.69, 0.86, 0.80\n",
    "    ],\n",
    "    \"TF-IDF\": [\n",
    "        0.82, 0.82, 0.82,\n",
    "        0.94, 0.79, 0.70, 0.79, 0.93,\n",
    "        0.91, 0.84, 0.68, 0.86, 0.81\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the table\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next steps (for said)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You're in a great spot to move to Step 2: Pretrained Word Embeddings. Here's what you could try next:\n",
    "\n",
    "- Replace BoW with averaged GloVe or word2vec embeddings for each abstract.\n",
    "\n",
    "- Use mean/sum/max pooling over word embeddings to create document vectors.\n",
    "\n",
    "- Feed those vectors into the same MLP structure for a fair comparison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Word Embeddings\n",
    "\n",
    "You can use pretrained embeddings such as GloVe or word2vec. In this example, we’ll use GloVe with a dimension of 300. First, download the GloVe file (e.g., glove.6B.300d.txt) if you haven’t already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_index = {}\n",
    "embedding_dim = 300  # Change this depending on which embedding file you use\n",
    "\n",
    "# Open the GloVe file (ensure the file is in your working directory)\n",
    "with open('glove.6B.300d.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefficients = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefficients\n",
    "        \n",
    "print(\"Loaded {} word vectors.\".format(len(embedding_index)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Converting Abstracts into Document Vectors\n",
    "\n",
    "Instead of using BoW, you now create a fixed-sized vector for each abstract by aggregating its word embeddings.\n",
    "\n",
    "a. Define a Function to Compute the Document Vector\n",
    "\n",
    "Here is a function that aggregates word embeddings from the tokens with your choice of pooling method. In this code, we demonstrate mean pooling. (You can change 'mean' to 'sum' or 'max' in the call if you prefer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(tokens, embedding_index, embedding_dim=300, pooling='mean'):\n",
    "    # Collect embeddings for words that are present in the pretrained dictionary\n",
    "    valid_vectors = [embedding_index[word] for word in tokens if word in embedding_index]\n",
    "    if not valid_vectors:\n",
    "        # If none of the words are in the embedding dictionary, return a zero vector\n",
    "        return np.zeros(embedding_dim)\n",
    "    valid_vectors = np.array(valid_vectors)\n",
    "    \n",
    "    if pooling == 'mean':\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    elif pooling == 'sum':\n",
    "        return np.sum(valid_vectors, axis=0)\n",
    "    elif pooling == 'max':\n",
    "        return np.max(valid_vectors, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported pooling type. Choose 'mean', 'sum', or 'max'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Create Document Vectors for Training and Test Sets\n",
    "\n",
    "Apply the function to each set of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "\n",
    "X_train = np.vstack(train_df['tokens'].apply(lambda tokens: get_document_vector(tokens, embedding_index, embedding_dim, pooling='mean')))\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "# For test data\n",
    "X_test = np.vstack(test_df['tokens'].apply(lambda tokens: get_document_vector(tokens, embedding_index, embedding_dim, pooling='mean')))\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert String Labels to Integers\n",
    "\n",
    "You can use LabelEncoder from scikit-learn to transform your string labels into integers.\n",
    "a. Import and Apply LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4, 'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on your training labels and transform them into integers\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)  # Transform test labels using the same encoder\n",
    "\n",
    "# Optionally, check the mapping\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building and Training the MLP Classifier\n",
    "\n",
    "Now that you have your fixed-size document vectors, you can train an MLP. Here’s an example using TensorFlow/Keras.\n",
    "\n",
    "a. Prepare the Data\n",
    "\n",
    "If you’re doing multi-class classification, convert your labels into categorical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "\n",
    "# Determine the number of classes using the unique classes from the encoder\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "y_train_cat = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Define the Model Architecture\n",
    "\n",
    "Set up a simple feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alipc\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m38,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,434</span> (185.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,434\u001b[0m (185.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,434</span> (185.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,434\u001b[0m (185.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Equivalent references\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "model = Sequential()\n",
    "# First hidden layer with 128 neurons\n",
    "model.add(Dense(128, activation='relu', input_shape=(embedding_dim,)))\n",
    "# Second hidden layer with 64 neurons\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Output layer with softmax activation\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Train the Model\n",
    "\n",
    "Train on your training data and validate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6305 - loss: 1.0941 - val_accuracy: 0.7469 - val_loss: 0.7149\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7511 - loss: 0.7053 - val_accuracy: 0.7538 - val_loss: 0.6909\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7592 - loss: 0.6756 - val_accuracy: 0.7694 - val_loss: 0.6494\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7731 - loss: 0.6401 - val_accuracy: 0.7679 - val_loss: 0.6509\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7756 - loss: 0.6250 - val_accuracy: 0.7697 - val_loss: 0.6391\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7833 - loss: 0.6042 - val_accuracy: 0.7667 - val_loss: 0.6507\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7851 - loss: 0.5986 - val_accuracy: 0.7752 - val_loss: 0.6238\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7909 - loss: 0.5866 - val_accuracy: 0.7832 - val_loss: 0.6089\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7954 - loss: 0.5747 - val_accuracy: 0.7860 - val_loss: 0.5974\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7973 - loss: 0.5630 - val_accuracy: 0.7831 - val_loss: 0.6075\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test, y_test_cat))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Evaluating the Classifier\n",
    "\n",
    "After training, evaluate the model’s performance on unseen data.\n",
    "\n",
    "a. Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7804 - loss: 0.6121\n",
      "Test Accuracy: 0.7831000089645386\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Generate a Detailed Classification Report\n",
    "\n",
    "For additional metrics like precision, recall, and F1-score, use scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      2013\n",
      "           1       0.78      0.74      0.76      2058\n",
      "           2       0.74      0.60      0.66      1995\n",
      "           3       0.65      0.86      0.74      1948\n",
      "           4       0.85      0.91      0.87      1990\n",
      "           5       0.86      0.81      0.83      2019\n",
      "           6       0.89      0.75      0.81      2042\n",
      "           7       0.68      0.58      0.62      1977\n",
      "           8       0.80      0.85      0.82      2013\n",
      "           9       0.74      0.82      0.78      1945\n",
      "\n",
      "    accuracy                           0.78     20000\n",
      "   macro avg       0.79      0.78      0.78     20000\n",
      "weighted avg       0.79      0.78      0.78     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Now, compare the encoded ground-truth and predicted labels\n",
    "print(classification_report(y_test_encoded, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
