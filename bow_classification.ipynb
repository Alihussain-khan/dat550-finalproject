{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('arxiv_train.csv')\n",
    "test_df = pd.read_csv('arxiv_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting preprocessing you lame ass nigga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train missing values:\n",
      " Unnamed: 0    0\n",
      "abstract      0\n",
      "label         0\n",
      "dtype: int64\n",
      "Test missing values:\n",
      " Unnamed: 0    0\n",
      "abstract      0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train missing values:\\n\", train_df.isnull().sum())\n",
    "print(\"Test missing values:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['abstract', 'label'], inplace=True)\n",
    "test_df.dropna(subset=['abstract', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing extra spaces, new lines, tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_df['clean_abstract'] = train_df['abstract'].apply(clean_text)\n",
    "test_df['clean_abstract'] = test_df['abstract'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'abstract', 'label', 'clean_abstract'], dtype='object')\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "train_df.head()\n",
    "print(train_df.columns)\n",
    "# Check the type of the first element in the 'clean_abstract' column\n",
    "print(type(train_df['clean_abstract'].iloc[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Apply the Tokenization to Your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m tokens\n\u001b[0;32m     13\u001b[0m \u001b[39m# Apply the tokenizer to your DataFrame\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39;49m\u001b[39mclean_abstract\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(simple_tokenize)\n\u001b[0;32m     15\u001b[0m test_df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_df[\u001b[39m'\u001b[39m\u001b[39mclean_abstract\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(simple_tokenize)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Verify by checking the new 'tokens' column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[142], line 3\u001b[0m, in \u001b[0;36msimple_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msimple_tokenize\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m    Tokenizes the input text by converting to lowercase and extracting words.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m    Returns a list of tokens.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()  \u001b[39m# convert to lowercase\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by converting to lowercase and extracting words.\n",
    "    Returns a list of tokens.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    # use re.findall to grab sequences of word characters (letters/numbers)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text) \n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenizer to your DataFrame\n",
    "train_df['tokens'] = train_df['clean_abstract'].apply(simple_tokenize)\n",
    "test_df['tokens'] = test_df['clean_abstract'].apply(simple_tokenize)\n",
    "\n",
    "# Verify by checking the new 'tokens' column\n",
    "print(train_df[['clean_abstract', 'tokens']].head())\n",
    "print(type(train_df['clean_abstract'].iloc[0]))\n",
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoding labels as integers because its a multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4, 'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# Save the mapping for later use\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stratified Train/Dev Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      " label_encoded\n",
      "9    0.100687\n",
      "3    0.100656\n",
      "7    0.100281\n",
      "4    0.100125\n",
      "2    0.100062\n",
      "0    0.099844\n",
      "8    0.099828\n",
      "5    0.099766\n",
      "6    0.099469\n",
      "1    0.099281\n",
      "Name: proportion, dtype: float64\n",
      "Dev label distribution:\n",
      " label_encoded\n",
      "9    0.100687\n",
      "3    0.100625\n",
      "7    0.100312\n",
      "4    0.100125\n",
      "2    0.100062\n",
      "8    0.099875\n",
      "0    0.099812\n",
      "5    0.099750\n",
      "6    0.099500\n",
      "1    0.099250\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split on the encoded labels\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    train_df['clean_abstract'],\n",
    "    train_df['label_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label_encoded']\n",
    ")\n",
    "\n",
    "# Optional sanity check: label distribution\n",
    "print(\"Train label distribution:\\n\", train_labels.value_counts(normalize=True))\n",
    "print(\"Dev label distribution:\\n\", dev_labels.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels.values\n",
    "y_dev = dev_labels.values\n",
    "label_map = {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4,\n",
    "             'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1 : text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basic count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer shape (train): (64000, 10000)\n",
      "CountVectorizer shape (dev): (16000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_count = count_vectorizer.fit_transform(train_texts)\n",
    "X_dev_count = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "print(\"CountVectorizer shape (train):\", X_train_count.shape)\n",
    "print(\"CountVectorizer shape (dev):\", X_dev_count.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (64000, 10000)\n",
      "TF-IDF shape (dev): (16000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(dev_texts)\n",
    "\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF shape (dev):\", X_dev_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting data to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this for BOW tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense and then to tensors\n",
    "X_train_tensor = torch.tensor(X_train_count.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_count.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run this for TF-IDF tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev_tfidf.toarray(), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- definfing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))  # Regularization\n",
    "            current_dim = h\n",
    "        layers.append(nn.Linear(current_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 836.6592\n",
      "Epoch 2/5 - Loss: 443.5131\n",
      "Epoch 3/5 - Loss: 337.8980\n",
      "Epoch 4/5 - Loss: 260.1777\n",
      "Epoch 5/5 - Loss: 197.4547\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FeedforwardNN(input_dim=10000, hidden_dims=[128, 64], num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results with BOW tensors \n",
    "- Epoch 1/5 - Loss: 706.6508\n",
    "- Epoch 2/5 - Loss: 390.9463\n",
    "- Epoch 3/5 - Loss: 277.9484\n",
    "- Epoch 4/5 - Loss: 196.3589\n",
    "- Epoch 5/5 - Loss: 142.3870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rusults with TF-IDF tensors \n",
    "- Epoch 1/5 - Loss: 831.5841\n",
    "- Epoch 2/5 - Loss: 439.9397\n",
    "- Epoch 3/5 - Loss: 336.5091\n",
    "- Epoch 4/5 - Loss: 258.9744\n",
    "- Epoch 5/5 - Loss: 196.4470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.95      0.92      0.94      1597\n",
      "    cond-mat       0.79      0.81      0.80      1588\n",
      "          cs       0.72      0.67      0.69      1601\n",
      "        eess       0.76      0.82      0.79      1610\n",
      "      hep-ph       0.93      0.93      0.93      1602\n",
      "      hep-th       0.90      0.91      0.90      1596\n",
      "        math       0.85      0.84      0.85      1592\n",
      "     physics       0.68      0.68      0.68      1605\n",
      "    quant-ph       0.87      0.86      0.87      1598\n",
      "        stat       0.80      0.82      0.81      1611\n",
      "\n",
      "    accuracy                           0.82     16000\n",
      "   macro avg       0.82      0.82      0.82     16000\n",
      "weighted avg       0.82      0.82      0.82     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in dev_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compraing BOW and TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Metric  CountVectorizer  TF-IDF\n",
      "   Accuracy             0.83    0.82\n",
      "   Macro F1             0.83    0.82\n",
      "Weighted F1             0.83    0.82\n",
      "Astro-ph F1             0.93    0.94\n",
      "Cond-mat F1             0.80    0.79\n",
      "      CS F1             0.70    0.70\n",
      "    EESS F1             0.79    0.79\n",
      "  Hep-ph F1             0.93    0.93\n",
      "  Hep-th F1             0.91    0.91\n",
      "    Math F1             0.85    0.84\n",
      " Physics F1             0.69    0.68\n",
      "Quant-ph F1             0.86    0.86\n",
      "    Stat F1             0.80    0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the metrics for both models\n",
    "data = {\n",
    "    \"Metric\": [\n",
    "        \"Accuracy\", \"Macro F1\", \"Weighted F1\", \n",
    "        \"Astro-ph F1\", \"Cond-mat F1\", \"CS F1\", \"EESS F1\", \"Hep-ph F1\", \n",
    "        \"Hep-th F1\", \"Math F1\", \"Physics F1\", \"Quant-ph F1\", \"Stat F1\"\n",
    "    ],\n",
    "    \"CountVectorizer\": [\n",
    "        0.83, 0.83, 0.83,\n",
    "        0.93, 0.80, 0.70, 0.79, 0.93,\n",
    "        0.91, 0.85, 0.69, 0.86, 0.80\n",
    "    ],\n",
    "    \"TF-IDF\": [\n",
    "        0.82, 0.82, 0.82,\n",
    "        0.94, 0.79, 0.70, 0.79, 0.93,\n",
    "        0.91, 0.84, 0.68, 0.86, 0.81\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the table\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next steps (for said)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You're in a great spot to move to Step 2: Pretrained Word Embeddings. Here's what you could try next:\n",
    "\n",
    "- Replace BoW with averaged GloVe or word2vec embeddings for each abstract.\n",
    "\n",
    "- Use mean/sum/max pooling over word embeddings to create document vectors.\n",
    "\n",
    "- Feed those vectors into the same MLP structure for a fair comparison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Word Embeddings\n",
    "\n",
    "You can use pretrained embeddings such as GloVe or word2vec. In this example, we’ll use GloVe with a dimension of 300. First, download the GloVe file (e.g., glove.6B.300d.txt) if you haven’t already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_index = {}\n",
    "embedding_dim = 300  # Change this depending on which embedding file you use\n",
    "\n",
    "# Open the GloVe file (ensure the file is in your working directory)\n",
    "with open('glove.6B.300d.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefficients = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefficients\n",
    "        \n",
    "print(\"Loaded {} word vectors.\".format(len(embedding_index)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Converting Abstracts into Document Vectors\n",
    "\n",
    "Instead of using BoW, you now create a fixed-sized vector for each abstract by aggregating its word embeddings.\n",
    "\n",
    "a. Define a Function to Compute the Document Vector\n",
    "\n",
    "Here is a function that aggregates word embeddings from the tokens with your choice of pooling method. In this code, we demonstrate mean pooling. (You can change 'mean' to 'sum' or 'max' in the call if you prefer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(tokens, embedding_index, embedding_dim=300, pooling='mean'):\n",
    "    # Collect embeddings for words that are present in the pretrained dictionary\n",
    "    valid_vectors = [embedding_index[word] for word in tokens if word in embedding_index]\n",
    "    if not valid_vectors:\n",
    "        # If none of the words are in the embedding dictionary, return a zero vector\n",
    "        return np.zeros(embedding_dim)\n",
    "    valid_vectors = np.array(valid_vectors)\n",
    "    \n",
    "    if pooling == 'mean':\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    elif pooling == 'sum':\n",
    "        return np.sum(valid_vectors, axis=0)\n",
    "    elif pooling == 'max':\n",
    "        return np.max(valid_vectors, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported pooling type. Choose 'mean', 'sum', or 'max'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Create Document Vectors for Training and Test Sets\n",
    "\n",
    "Apply the function to each set of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "\n",
    "X_train = np.vstack(train_df['tokens'].apply(lambda tokens: get_document_vector(tokens, embedding_index, embedding_dim, pooling='mean')))\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "# For test data\n",
    "X_test = np.vstack(test_df['tokens'].apply(lambda tokens: get_document_vector(tokens, embedding_index, embedding_dim, pooling='mean')))\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert String Labels to Integers\n",
    "\n",
    "You can use LabelEncoder from scikit-learn to transform your string labels into integers.\n",
    "a. Import and Apply LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'eess': 3, 'hep-ph': 4, 'hep-th': 5, 'math': 6, 'physics': 7, 'quant-ph': 8, 'stat': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on your training labels and transform them into integers\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)  # Transform test labels using the same encoder\n",
    "\n",
    "# Optionally, check the mapping\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building and Training the MLP Classifier\n",
    "\n",
    "Now that you have your fixed-size document vectors, you can train an MLP. Here’s an example using TensorFlow/Keras.\n",
    "\n",
    "a. Prepare the Data\n",
    "\n",
    "If you’re doing multi-class classification, convert your labels into categorical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "\n",
    "# Determine the number of classes using the unique classes from the encoder\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "y_train_cat = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Define the Model Architecture\n",
    "\n",
    "Set up a simple feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m38,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,434</span> (185.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,434\u001b[0m (185.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,434</span> (185.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,434\u001b[0m (185.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Equivalent references\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "model = Sequential()\n",
    "# First hidden layer with 128 neurons\n",
    "model.add(Dense(128, activation='relu', input_shape=(embedding_dim,)))\n",
    "# Second hidden layer with 64 neurons\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Output layer with softmax activation\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Train the Model\n",
    "\n",
    "Train on your training data and validate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6243 - loss: 1.1116 - val_accuracy: 0.7506 - val_loss: 0.7117\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 0.7044 - val_accuracy: 0.7551 - val_loss: 0.6895\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7644 - loss: 0.6662 - val_accuracy: 0.7677 - val_loss: 0.6489\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.6447 - val_accuracy: 0.7639 - val_loss: 0.6471\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7766 - loss: 0.6231 - val_accuracy: 0.7753 - val_loss: 0.6279\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.6094 - val_accuracy: 0.7769 - val_loss: 0.6228\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7888 - loss: 0.5910 - val_accuracy: 0.7798 - val_loss: 0.6081\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7895 - loss: 0.5837 - val_accuracy: 0.7814 - val_loss: 0.6126\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7964 - loss: 0.5668 - val_accuracy: 0.7783 - val_loss: 0.6131\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8002 - loss: 0.5574 - val_accuracy: 0.7762 - val_loss: 0.6273\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test, y_test_cat))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Evaluating the Classifier\n",
    "\n",
    "After training, evaluate the model’s performance on unseen data.\n",
    "\n",
    "a. Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7728 - loss: 0.6316\n",
      "Test Accuracy: 0.776199996471405\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Generate a Detailed Classification Report\n",
    "\n",
    "For additional metrics like precision, recall, and F1-score, use scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89      2013\n",
      "           1       0.74      0.79      0.76      2058\n",
      "           2       0.74      0.58      0.65      1995\n",
      "           3       0.72      0.76      0.74      1948\n",
      "           4       0.93      0.82      0.87      1990\n",
      "           5       0.78      0.87      0.82      2019\n",
      "           6       0.74      0.88      0.81      2042\n",
      "           7       0.72      0.52      0.61      1977\n",
      "           8       0.87      0.78      0.82      2013\n",
      "           9       0.70      0.85      0.76      1945\n",
      "\n",
      "    accuracy                           0.78     20000\n",
      "   macro avg       0.78      0.78      0.77     20000\n",
      "weighted avg       0.78      0.78      0.77     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Now, compare the encoded ground-truth and predicted labels\n",
    "print(classification_report(y_test_encoded, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
